---
title:  "[파이썬] 웹페이지 컨텐츠 수집 - 데이터 수집(6)"
date: 2020-05-05
categories: [python]
---

## 1. 웹 페이지 컨텐츠 수집 개요

### 1) 웹 스크래핑

- 컴퓨터 소프트웨어 기술로 웹 사이트의 정보를 추출하는 것
- 국내에서 흔히 크롤링이라는 용어로 많이 알려짐.

### 2) 웹 크롤러

- 인터넷에 있는 웹 페이지를 방문해서 자료를 수집하는 프로그램

### 3) Python으로 크롤러를 구현하기 위해 필요한 모듈을 설치

- request : 웹 페이지 URL에 접속하여 컨텐츠를 가져오는 기능을 제공하는 모듈

```
pip install requests
```

- bs4(BeautifulSoup) : 웹 페이지의 HTML 구조를 분석해서 원하는 부분만 추출하는 기능을 제공하는 모듈

```
pip install bs4
```

### 4) 클롤링 주의사항

- 대법원 판례로 크롤링은 불법으로 명시하고 있다.

- 크롤링 한 데이터를 운영중인 서비스에 게시할 경우 법적 책임이 있을 수 있다.

<br>

## 2. 웹 페이지 컨텐츠를 수집하기 위한 준비사항

- 웹 페이지 소스코드 보기
    - explore나 chrome에서 F12 '페이지 소스보기'를 통해 소스코드를 볼 수 있다.

### 1) HTML 소스코드 확인

- 웹페이지는 HTML이라는 언어로 웹 개발자의 의해 작성된다.
- 웹 페이지에서 원하는 컨텐츠를 추출하기 위해서는 웹 페이지를 구성하는 소스코드를 분석하고 그 안에서 의미있는 데이터를 추려내는 과정이 필요하다.
- HTML코드에 대한 지식을 갖추어야 한다.

### 2) HTML 코드 분석

- 크롬 브라우저로 수집을 원하는 웹페이지에 접속한 뒤 '개발자도구'를 실행한다.
    = 대부분의 웹 페이지들은 표현형식이 동일한 경우 비슷한 HTML 코드 구조를 갖기 때문에 대표 페이지 하나만 분석하면 다른 내용을 담고 있는 같은 형태의 웹 페이지들을 동일한 방법으로 처리할 수 있다.

### 3) HTML 코드 구조 분석

HTML 코드는 태그와 속성으로 구성되어 있다.

```HTML
<태그 속성 ="값" 속성="값"...>내용 </태그>
```

- 예시

```html
<div id="abc" style="-webkit-tap-highlight-color: rgba(0,0,0,0);">
...
</div>
```

### 4) Python에서 인지할 수 있는 HTML 구조

기본적으로 CSS에서 사용하는 선택자(selector) 구조를 사용한다.

- id 속성값은 #을 붙여서 접근
- class 속성은 점(.)을 붙여서 접근한다.
- 태그 이름은 동일한 값으로 접근한다.

#### 수집을 원하는 부분을 구성하는 태그에 id 속성값이 있는 경우

- id는 그 페이지에서 고유한 영역을 의미하기 때문에 id값을 사용해 특정 부분에 직접 접근이 가능하다.

- id 값에 '#'을 적용한 값을 사용한다.

#### 수집을 원하는 부분을 구성하는 태그에 class 속성값이 있는 경우

- class 속성은 그 영역의 디자인 요소를 연결하기 위한 속성이므로 비슷한 디자인 특성을 갖는 모든 영역에서 재사용되는 값이다.

- 그러므로 class 속성 하나만으로 수집을 원하는 부분을 특정 지을 수 없기 때문에 그 부분을 구성하는 코드의 2 ~ 3 deptho 구조를 확인해야 한다.

### 5) User Agent 확인하기

> 웹 브라우저가 웹 사이트(서버)에 전송하는 브라우저 및 운영체제의 버전 정보를 담고 있는 문자열

- 크롬 개발자 도구에서 User Agent확인
    - 파이썬으로 개발한 크롤러는 웹 브라우저가 아니므로 User Agent값이 없다.
    - 보안처리가 적용된 웹 서버는 **User Agent 값이 없을 경우 비정상 접속으로 간주**하여 정상적인 결과를 출력하지 않는 경우가 있다.

<br>

## 3. 뉴스 기사 크롤링 연습

- 샘플 페이지와 이미지 주소 및 UserAgent값 정의

```python
#-----------------------------------------------------
# sample.py
#-----------------------------------------------------
# 웹 브라우저 버전 정보
user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36"

# 네이버 뉴스 기사
naver_news_url = "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=092&aid=0002165157"

# 네이버 이미지 검색 결과 중 하나
image_url = 'http://blogfiles.naver.net/20111129_220/ktr38_1322533255591QJtWz_JPEG/s_1210_2011112414293493.jpg'
```

- 구현

```python
# -*- coding: utf-8 -*-

# 필요한 모듈 참조하기
import requests                     # -> 웹 페이지 요청 모듈
from bs4 import BeautifulSoup       # -> 웹 페이지 소스코드 분석 모듈
from print_df import print_df       # -> 데이터 출력 모듈

# sample.py에서 필요한 데이터 참조
from sample import naver_news_url   # 가져올 뉴스의 URL
from sample import user_agent       # 웹 브라우저 버전 정보

#----------------------------------------------------------
# 데이터 수집 - 웹 페이지 HTML 소스코드 가져오기
#----------------------------------------------------------
# 접속 세션을 생성
# 세션 = 클라이언트(브라우저)와 서버(웹사이트)간의 연결단위
# -> 이 객체에 접속에 필요한 기본 정보를 설정한다.
session = requests.Session()
# 현재 세션의 referer 페이지를 '없음'으로 강제 설정
# -> referer = 이전에 머물렀던 페이지 주소
# -> referer값이 없으면 웹 서버는 브라우저에서 직접 URL을 입력한 것으로 간주한다.
# 현재 세션의 웹 브라우저 정보(User-agent)를 구글 크롬으로 설정
session.headers.update( {'referer': None, 'User-agent': user_agent} )

# 특정 웹 페이지에 접속
# -> headers 파라미터로 가져올 컨텐츠의 형식을 미리 지정해 놓는다.
r = session.get(naver_news_url)

# -> 결과확인
if r.status_code != 200:
    print("%d 에러가 발생했습니다." % r.status_code)
    # 즉시 종료
    quit()

# 가져온 HTML 코드 확인
# -> 웹 페이지의 인코딩 형식을 확인하여 설정해야 한다.
r.encoding = "euc-kr"
print_df(r.text)


#----------------------------------------------------------
# 데이터 전처리 (1) - HTML을 분석하여 원하는 영역 추출
#----------------------------------------------------------
# 웹 페이지의 소스코드 HTML 분석 객체로 생성
soup = BeautifulSoup(r.text, 'html.parser')

# CSS 선택자를 활용하여 가져오기를 원하는 부분 지정
selector = soup.select('#articleBodyContents')

if not selector:   # 가져온 내용이 없다면?
    print("뉴스기사 크롤링 실패")
    quit()

print_df(selector)


#----------------------------------------------------------
# 데이터 전처리 (2) - 추출된 영역 안에서 불필요한 태그 제거
#----------------------------------------------------------
# html의 id속성을 통해 가져온 원소는 그 페이지 내에서 고유한 영역을 의미하므로,
# select() 함수의 결과가 list라 하더라도 실제 원소는 단 하나만 존재한다.
# 그렇기 때문에 리스트에 대한 0번째 요소에 직접 접근해도 무관하다.
item = selector[0]

# item에 포함된 불필요한 태그들 제거
for target in item.find_all('script'):
    target.extract()

for target in item.find_all('a'):
    target.extract()

for target in item.find_all('br'):
    target.extract()

for target in item.find_all('span', {'class': 'end_photo_org'}):
    target.extract()

print_df(item)

#----------------------------------------------------------
# 최종 결과값 확인
#----------------------------------------------------------
# item 객체의 text속성에 대해 앞뒤 공백을 제거한 값을 리턴받는다.
result_str = item.text.strip()
print_df(result_str)
```

- 결과

```
<class 'str'>
    '프로젝트 카이퍼' 첫 위성 미국 연방통신위원회 심의 중(지디넷코리아=남혁우 기자)아마존이 저궤도 위성 3천200여 개를 띄워 전 세계 광대역 인터넷망을 구축하는 '프로젝트 카이퍼'를 본격화한다숀 레이 AWS 아시아 태평양지역 디벨로퍼 릴레이션 총괄은 28일 서울 중구 웨스틴조선호텔에서 열린 간담회에서 프로젝트 카이퍼 계획에 대해 소개했다.숀 레이 총괄은 “현재 프로젝트 카이퍼의 첫 번째 위성이 미국 연방통신위원회(FCC)심의를 받고 있다”며 “2022년까지 프로젝트 카이퍼를 완수할 계획”이라고 밝혔다.프로젝트 카이퍼는 저궤도 위성을 활용해 전 세계에 지연 없는 초고속 광대역 인터넷망을 제공하기 위한 시도다.숀 레이 총괄은 “사막, 대양 등 인터넷 접속이 어려운 환경에서 이뤄지는 광산업, 석유가스 발굴 시추산업 등을 비롯해 내전 등으로 인터넷이 강제적으로 끊긴 환경에서도 프로젝트 카이퍼를 활용될 수 있을 것"이라고 설명했다.AWS 외에 스페이스엑스도 2천200 대의 궤도위성을 쏘아 올리는 등 조만간 민간 위성 시대가 본격화될 전망이다. 숀 레이 총괄이 발표한 내용에 따르면 앞으로 10년 간 약 2만5천여대의 민간 위성이 띄워질 예정이다.민간 위성이 대폭 늘어나면서 이를 활용한 인터넷 서비스도 본격화될 전망이다. AWS는 쏘아 올린 3천여개 소형위성으로 구축한 인터넷망을 활용해 바다 위의 화물선 위치뿐 아니라 화물선에 실린 품목까지 상세하게 파악하는 서비스도 제공할 계획이다.이와 함께 AWS는 위성을 통해 확보한 많은 양의 데이터 처리 효율을 높이고 부하를 줄이기 위해 엣지컴퓨팅 단계에서부터 데이터를 분석하고 저장할 수 있도록 AWS 그린그래스와 AWS IoT를 지원한다.
```

<br>

## 4. 이미지 다운로드 받기

- 구현 코드

```python
# -*- coding: utf-8 -*-

# 필요한 모듈 참조하기
import os                           # -> os모듈 (파일경로 처리용)
import requests                     # -> 웹 페이지 요청 모듈
from print_df import print_df       # -> 데이터 출력 모듈

# sample.py에서 필요한 데이터 참조
from sample import image_url        # 다운로드 할 이미지의 URL
from sample import user_agent       # 웹 브라우저 버전 정보

#----------------------------------------------------------
# 데이터 수집 - 이미지 URL 가져오기
#----------------------------------------------------------
# 접속 세션을 생성하고 referer 페이지와 브라우저 버전 설정
session = requests.Session()
session.headers.update( {'referer': None, 'User-agent': user_agent} )

# 이미지 URL에 접속 (파일 다운로드의 경우 stream=True 파라미터 추가)
r = session.get(image_url, stream=True)

# -> 결과확인 후 에러 발생시 강제 종료
if r.status_code != 200:
    print("%d 에러가 발생했습니다." % r.status_code)
    quit()


#----------------------------------------------------------
# 데이터 전처리 - 가져온 결과에서 바이너리를 추출하여 저장하기
#----------------------------------------------------------
# 이미지 주소에서 파일이름만 추출하기 -> s_1210_2011112414293493.jpg
fname = os.path.basename(image_url)
print_df(fname)

# 파일명에서 마지막 점(.)의 위치를 찾음
p = fname.rfind(".")

# 찾아진 위치가 0보다 작다면? --> 파일이름에 점이 없다면?
if p < 0:
	# 파일이름 뒤에 강제로 .jpg 라고 확장자 적용
    fname += ".jpg"

print_df(fname)

# 이미지의 바이너리(이진값) 데이터를 추출
img = r.raw.read()

# 추출한 데이터를 저장
# -> 'w': 텍스트 쓰기 모드, 'wb': 바이너리(이진값) 쓰기 모드
with open(fname, 'wb') as f:
    f.write(img)
```

![이미지 복사](/assets/Images/python/chapter44/image1.JPG)

참조 : 호쌤의 교육자료, (<https://blog.itpaper.co.kr/>)